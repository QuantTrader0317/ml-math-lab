# ğŸ§  Machine Learning Math Lab â€” 6 Core Projects in Python (Built from Scratch)

### ğŸš€ Overview
This project is my hands-on lab for mastering the **mathematical foundations of machine learning** using pure Python (v3.13) and NumPy.

Everything here was written, debugged, and explained line by line inside **PyCharm**, to fully understand *how the math behind each model actually works* â€” not just how to call it from a library.

---

## ğŸ“‚ Projects Included

| # | Project | Core Concept | Key Skill |
|:-:|:--|:--|:--|
| 1 | **Power Iteration** | Eigenvalues & Eigenvectors | Linear Algebra Fundamentals |
| 2 | **Ordinary Least Squares (OLS)** | Linear Regression | Matrix Inversion & RÂ² Analysis |
| 3 | **Logistic Regression (Gradient Descent)** | Classification | Optimization & Cost Functions |
| 4 | **k-Means Clustering** | Unsupervised Learning | Distance Metrics & Iterative Refinement |
| 5 | **Principal Component Analysis (PCA)** | Dimensionality Reduction | SVD, Variance, & Reconstruction |
| 6 | **Regularized Regression (Ridge & Lasso)** | Overfitting Control | L2 & L1 Penalties, Feature Selection |
| 7 | **Optimizer Comparison (SGD â€¢ Momentum â€¢ Adam)** | Optimization Algorithms | Convergence Speed & Stability |
| 8 | **Time-Series Analysis (Stationarity & Autocorrelation)** | Statistical Properties of Market Data | ADF Tests, ACF, Ljungâ€“Box, AR(1) Model |
| 9 | **Feature Engineering for Returns** | Quant Signal Design | Momentum, Volatility, Z-Scores, Rolling Beta, Lagged Returns |

---

### ğŸ’¡ What I Learned
- How to **translate math formulas into working code**  
- Why each algorithm converges (or fails)  
- The difference between **exact math** and **numerical stability**  
- How modern libraries like scikit-learn are built under the hood  
- How **regularization stabilizes models** and performs **automatic feature selection**

---

### âš™ï¸ Tools Used
- Python 3.13  
- NumPy 2.1+  
- scikit-learn (for cross-verification)  
- PyCharm (for daily environment setup, debugging, and testing)

---

### ğŸ§© Example Insights
- **Power Iteration:** a matrix naturally â€œpullsâ€ vectors toward its strongest direction  
- **OLS Regression:** fitting a line is just solving `Xáµ€XÎ² = Xáµ€y`  
- **Logistic Regression:** prediction is probability, not just classification  
- **k-Means:** patterns form through repeated nearest-center updates  
- **PCA:** you can compress data while keeping most of its meaning  
- **Ridge & Lasso:** adding penalties keeps models stable and highlights the most important features  
- **Optimizers (SGD vs Momentum vs Adam):** All train the same logistic model differently.  
  - SGD â†’ simple but noisy steps  
  - Momentum â†’ adds velocity for smoother progress  
  - Adam â†’ adapts learning rate per weight, usually fastest and most stable  
  These control *how models actually learn*, which is crucial for larger ML & quant research systems.
- **Time-Series Analysis:** Prices are non-stationary (random-walk behavior); returns are stationary.  
  Weak negative autocorrelation indicates short-term mean reversion â€” a foundation for quant forecasting.  
- **Feature Engineering for Returns:** Built momentum, volatility, z-score, beta, and lagged-return features.  
  Individually weak but collectively form the basis of a multi-factor predictive signal.

---

### ğŸ Key Takeaway
> â€œIf you can code the math from scratch, you can understand any ML model â€” no black boxes.â€

This repo built my foundation for deeper research in **quantitative finance, econometrics, and machine learning.**

---

### ğŸ“¸ Optional Demo Post
> Just finished building 6 core ML algorithms completely from scratch in Python.  
> No high-level wrappers, no shortcuts â€” just math, NumPy, and logic.  
> The goal wasnâ€™t just to get the right answer, but to *understand why* itâ€™s right.  
>
> ğŸ§® Power Iteration â†’ Eigenvalues  
> ğŸ“ˆ OLS Regression â†’ Linear Models  
> ğŸ” Logistic Regression â†’ Optimization  
> ğŸ¯ k-Means â†’ Clustering  
> ğŸ” PCA â†’ Dimensionality Reduction  
> âš–ï¸ Ridge & Lasso â†’ Regularization & Feature Selection  
>
> All coded and explained inside PyCharm using Python 3.13.  
> This is how you turn theory into intuition.

---

### ğŸ“ Folder Layout
ml-math-lab/
â”‚
â”œâ”€â”€ projects/
â”‚ â”œâ”€â”€ 1_power_iteration.py
â”‚ â”œâ”€â”€ 2_ols_from_scratch.py
â”‚ â”œâ”€â”€ 3_logreg_scratch.py
â”‚ â”œâ”€â”€ 4_kmeans_scratch.py
â”‚ â”œâ”€â”€ 5_pca_svd.py
â”‚ â””â”€â”€ 6_regularized_regression.py
â”‚ â””â”€â”€ 7_optimizers_logreg.py
â”‚ â””â”€â”€ 8_time_series_basics.py
â”‚ â””â”€â”€ 9_feature_engineering_returns.py
â”œâ”€â”€ README.md â† this file
â””â”€â”€ requirements.txt

---

### âœ… Next Step Ideas
- **Project 7:** Optimizers (SGD, Momentum, Adam)  
- **Project 8:** Time-Series Basics for Returns  
- **Project 9:** Feature Engineering for Quant Signals  
- **Project 10:** Backtester with Transaction Costs  
- **Project 11:** Portfolio Construction (Ridge/Volatility Targeting)  
- **Project 12:** Broker API Integration (Paper Trading)  
- **Project 13:** Risk & Monitoring Dashboard  

---

**Created by:** Clyde Williams Jr.  
**Focus:** Quantitative Finance â€¢ Machine Learning â€¢ Mathematical Research  
**Stack:** Python | NumPy | Statistics | Linear Algebra
